#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºç¡€çˆ¬è™«ç±»
"""

import requests
import re
import time
import os
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from playwright.sync_api import sync_playwright

from src.config.settings import *
from src.config.websites import *
from src.utils.logger import get_logger
from src.core.protocol_converter import get_converter
from src.core.exceptions import (
    CollectorDisabledError,
    ArticleLinkNotFoundError,
    SubscriptionLinkNotFoundError,
    NetworkError,
    RequestTimeoutError,
    ConnectionError as V2RayConnectionError,
    ProxyError,
)


class BaseCollector(ABC):
    """åŸºç¡€çˆ¬è™«æŠ½è±¡ç±»"""

    def __init__(self, site_config):
        self.site_config = site_config
        self.site_name = site_config["name"]
        self.base_url = site_config["url"]
        self.enabled = site_config.get("enabled", True)
        self.last_article_url = None  # è®°å½•æœ€åè®¿é—®çš„æ–‡ç« URL

        # è®¾ç½®æ—¥å¿—
        self.logger = get_logger(f"collector.{self.site_name}")

        # åˆ›å»ºä¼šè¯
        self.session = requests.Session()

        # æ·»åŠ æ›´çœŸå®çš„è¯·æ±‚å¤´ä»¥ç»•è¿‡åçˆ¬è™«
        # åœ¨GitHub Actionsç¯å¢ƒä¸­ä½¿ç”¨éšæœºçœŸå®çš„User-Agentå’Œæ›´å®Œæ•´çš„è¯·æ±‚å¤´
        import os
        import random

        real_user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

        if os.getenv("GITHUB_ACTIONS") == "true":
            browser_ua = random.choice(real_user_agents)
        else:
            browser_ua = USER_AGENT

        self.session.headers.update(
            {
                "User-Agent": browser_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
                "DNT": "1",
                "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"Windows"',
                "sec-ch-ua-arch": '"x86"',
                "sec-ch-ua-bitness": '"64"',
                "sec-ch-ua-full-version": '"120.0.6099.109"',
                "sec-ch-ua-full-version-list": '"Not_A Brand";v="8.0.0.0", "Chromium";v="120.0.6099.109", "Google Chrome";v="120.0.6099.109"',
                "sec-ch-ua-model": '""',
                "sec-ch-ua-platform-version": '"15.0.0"',
            }
        )

        if os.getenv("GITHUB_ACTIONS") == "true":
            import time
            import random

            time.sleep(random.uniform(2, 4))

        # ç¦ç”¨SSLéªŒè¯ï¼ˆä¸ä»£ç†ä½¿ç”¨ä¿æŒä¸€è‡´ï¼‰
        self.session.verify = False

        # é…ç½®ä»£ç†ï¼ˆå¦‚æœç³»ç»Ÿæœ‰è®¾ç½®ä»£ç†ï¼‰
        import os
        from src.config.websites import BROWSER_ONLY_SITES

        http_proxy = os.getenv("http_proxy") or os.getenv("HTTP_PROXY")
        https_proxy = os.getenv("https_proxy") or os.getenv("HTTPS_PROXY")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¦ç”¨ä»£ç†ï¼ˆä½¿ç”¨æµè§ˆå™¨ç›´è¿è®¿é—®ï¼‰
        site_key = self.site_config.get("collector_key", self.site_config.get("name"))
        if site_key in BROWSER_ONLY_SITES:
            self.logger.info(f"âš ï¸ {self.site_name} ä½¿ç”¨æµè§ˆå™¨ç›´è¿è®¿é—®ï¼ˆç¦ç”¨ä»£ç†ï¼‰")
            http_proxy = None
            https_proxy = None

        # è®¾ç½®sessionä»£ç†
        if http_proxy or https_proxy:
            self.session.proxies = {
                "http": http_proxy,
                "https": https_proxy,
            }
            self.logger.info(
                f"âœ… å·²è®¾ç½®ä»£ç† - HTTP: {http_proxy}, HTTPS: {https_proxy}"
            )
            # éªŒè¯ä»£ç†è®¾ç½®
            self.logger.info(f"å½“å‰sessionä»£ç†è®¾ç½®: {self.session.proxies}")

            # æµ‹è¯•ä»£ç†è¿æ¥
            try:
                test_response = self.session.get(
                    "https://httpbin.org/ip", timeout=10, verify=False
                )
                if test_response.status_code == 200:
                    ip_info = test_response.json()
                    self.logger.info(
                        f"âœ… ä»£ç†è¿æ¥æµ‹è¯•æˆåŠŸï¼Œå½“å‰IP: {ip_info.get('origin', 'unknown')}"
                    )
                else:
                    self.logger.warning(
                        f"âš ï¸ ä»£ç†è¿æ¥æµ‹è¯•å¤±è´¥ï¼ŒçŠ¶æ€ç : {test_response.status_code}"
                    )
            except Exception as e:
                self.logger.warning(f"âš ï¸ ä»£ç†è¿æ¥æµ‹è¯•å¼‚å¸¸: {str(e)}")
        else:
            self.logger.info("âŒ æœªæ£€æµ‹åˆ°ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨ç›´è¿")

        # é…ç½®å‚æ•°
        self.timeout = REQUEST_TIMEOUT
        self.retry_count = REQUEST_RETRY
        self.delay = REQUEST_DELAY

        # å­˜å‚¨ç»“æœ
        self.collected_nodes = []
        self.subscription_links = []

        self.raw_data = ""

        # åè®®è½¬æ¢å™¨
        self.converter = get_converter(self.logger)

        # åˆå§‹åŒ–è¾…åŠ©å¤„ç†å™¨
        from src.core.handlers import RequestHandler, ArticleFinder, SubscriptionExtractor

        self.request_handler = RequestHandler(
            self.session, self.timeout, self.retry_count, self.logger
        )
        self.article_finder = ArticleFinder(
            self.base_url, self.site_name, self.logger, self.site_config
        )
        self.subscription_extractor = SubscriptionExtractor(
            self.logger, self.site_config, self.converter, MIN_NODE_LENGTH
        )

    def _make_request(self, url, method="GET", **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„è¯·æ±‚æ–¹æ³•ï¼Œæ”¯æŒä»£ç†å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°ç›´æ¥è¿æ¥"""
        return self.request_handler.make_request(url, method, **kwargs)

    def collect(self):
        """æ”¶é›†èŠ‚ç‚¹çš„ä¸»æ–¹æ³•"""
        if not self.enabled:
            self.logger.info(f"{self.site_name} å·²ç¦ç”¨ï¼Œè·³è¿‡æ”¶é›†")
            return []

        try:
            self.logger.info(f"å¼€å§‹æ”¶é›† {self.site_name} çš„èŠ‚ç‚¹")

            # è·å–æœ€æ–°æ–‡ç« URL
            article_url = self.get_latest_article_url()
            if not article_url:
                self.logger.warning(f"{self.site_name}: æœªæ‰¾åˆ°æœ€æ–°æ–‡ç« ")
                return []

            # è®°å½•æ–‡ç« URL
            self.last_article_url = article_url

            # æå–èŠ‚ç‚¹ä¿¡æ¯
            nodes = self.extract_nodes_from_article(article_url)

            # ä¿å­˜åŸå§‹æ•°æ®
            self.save_raw_data(article_url)

            self.logger.info(f"{self.site_name}: æ”¶é›†åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
            return nodes

        except requests.exceptions.Timeout as e:
            self.logger.error(f"{self.site_name}: è¯·æ±‚è¶…æ—¶ - {str(e)}")
            return []
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"{self.site_name}: è¿æ¥é”™è¯¯ - {str(e)}")
            return []
        except requests.exceptions.ProxyError as e:
            self.logger.error(f"{self.site_name}: ä»£ç†é”™è¯¯ - {str(e)}")
            return []
        except requests.exceptions.RequestException as e:
            self.logger.error(f"{self.site_name}: ç½‘ç»œè¯·æ±‚é”™è¯¯ - {str(e)}")
            return []
        except Exception as e:
            self.logger.error(f"{self.site_name}: æ”¶é›†å¤±è´¥ - {str(e)}")
            return []

    def collect_links(self) -> Dict[str, Any]:
        """
        é˜¶æ®µ1ï¼šåªæ”¶é›†æ–‡ç« é“¾æ¥å’Œè®¢é˜…é“¾æ¥ï¼Œä¸è§£æèŠ‚ç‚¹

        Returns:
            åŒ…å«æ–‡ç« URLå’Œè®¢é˜…é“¾æ¥çš„å­—å…¸
        """
        if not self.enabled:
            self.logger.info(f"{self.site_name} å·²ç¦ç”¨ï¼Œè·³è¿‡é“¾æ¥æ”¶é›†")
            return {}

        try:
            self.logger.info(f"å¼€å§‹æ”¶é›† {self.site_name} çš„é“¾æ¥")

            # è·å–æœ€æ–°æ–‡ç« URL
            article_url = self.get_latest_article_url()
            if not article_url:
                self.logger.warning(f"{self.site_name}: æœªæ‰¾åˆ°æœ€æ–°æ–‡ç« ")
                return {}

            # è®°å½•æ–‡ç« URL
            self.last_article_url = article_url

            # æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°ï¼ˆç›´æ¥è®¢é˜…é“¾æ¥ï¼‰
            if article_url.endswith("#direct_subscription"):
                subscription_url = article_url.replace("#direct_subscription", "")
                self.logger.info(f"æ£€æµ‹åˆ°ç›´æ¥è®¢é˜…é“¾æ¥: {subscription_url}")
                return {
                    "article_url": article_url,
                    "subscription_links": [subscription_url],
                    "raw_data": "",
                }

            # è®¿é—®æ–‡ç« é¡µé¢å¹¶æå–è®¢é˜…é“¾æ¥
            # å¯¹äº BROWSER_ONLY_SITESï¼Œç›´æ¥ä½¿ç”¨æµè§ˆå™¨è®¿é—®ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰
            from src.config.websites import BROWSER_ONLY_SITES

            site_key = self.site_config.get(
                "collector_key", self.site_config.get("name")
            )

            if site_key in BROWSER_ONLY_SITES:
                # ç›´æ¥ä½¿ç”¨æµè§ˆå™¨è®¿é—®ï¼ˆè·³è¿‡ä»£ç†å°è¯•ï¼‰
                self.logger.info(f"ä½¿ç”¨æµè§ˆå™¨è®¿é—®æ–‡ç« é¡µé¢: {article_url}")

                # ä¸´æ—¶ç¦ç”¨ä»£ç†
                original_proxies = self.session.proxies
                self.session.proxies = {"http": None, "https": None}

                try:
                    with sync_playwright() as p:
                        browser = p.chromium.launch(
                            headless=True,
                            args=["--no-sandbox", "--disable-dev-shm-usage"],
                        )
                        context = browser.new_context(
                            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            viewport={"width": 1920, "height": 1080},
                            locale="zh-CN",
                        )
                        page = context.new_page()
                        # ä½¿ç”¨ domcontentloaded é¿å… networkidle è¶…æ—¶
                        page.goto(
                            article_url, wait_until="domcontentloaded", timeout=90000
                        )
                        # ç­‰å¾…é¢å¤–æ—¶é—´è®©JSæ‰§è¡Œ
                        page.wait_for_timeout(5000)
                        content = page.content()
                        browser.close()

                    self.logger.info(f"æµè§ˆå™¨è®¿é—®æˆåŠŸï¼Œè·å–åˆ° {len(content)} å­—èŠ‚å†…å®¹")
                finally:
                    # æ¢å¤ä»£ç†è®¾ç½®
                    self.session.proxies = original_proxies
            else:
                response = self._make_request(article_url)
                content = response.text

            self.raw_data = content

            # æŸ¥æ‰¾è®¢é˜…é“¾æ¥
            subscription_links = self.find_subscription_links(content)
            self.subscription_links = subscription_links

            self.logger.info(
                f"{self.site_name}: æ‰¾åˆ° {len(subscription_links)} ä¸ªè®¢é˜…é“¾æ¥"
            )

            return {
                "article_url": article_url,
                "subscription_links": subscription_links,
                "raw_data": content,
            }

        except requests.exceptions.Timeout as e:
            self.logger.error(f"{self.site_name}: é“¾æ¥æ”¶é›†è¶…æ—¶ - {str(e)}")
            return {}
        except requests.exceptions.ConnectionError as e:
            self.logger.error(f"{self.site_name}: é“¾æ¥æ”¶é›†è¿æ¥é”™è¯¯ - {str(e)}")
            return {}
        except requests.exceptions.ProxyError as e:
            self.logger.error(f"{self.site_name}: é“¾æ¥æ”¶é›†ä»£ç†é”™è¯¯ - {str(e)}")
            return {}
        except requests.exceptions.RequestException as e:
            self.logger.error(f"{self.site_name}: é“¾æ¥æ”¶é›†ç½‘ç»œè¯·æ±‚é”™è¯¯ - {str(e)}")
            return {}
        except Exception as e:
            self.logger.error(f"{self.site_name}: é“¾æ¥æ”¶é›†å¤±è´¥ - {str(e)}")
            return {}

    def get_latest_article_url(self, target_date=None):
        """è·å–æ–‡ç« URLï¼Œæ”¯æŒæŒ‡å®šæ—¥æœŸ"""
        from src.config.websites import BROWSER_ONLY_SITES

        try:
            site_key = self.site_config.get(
                "collector_key", self.site_config.get("name")
            )
            use_browser_directly = site_key in BROWSER_ONLY_SITES

            self.logger.debug(
                f"site_key='{site_key}', BROWSER_ONLY_SITES={BROWSER_ONLY_SITES}, use_browser_directly={use_browser_directly}"
            )

            if use_browser_directly:
                self.logger.info(
                    f"ä½¿ç”¨æµè§ˆå™¨ç›´æ¥è®¿é—® (BROWSER_ONLY_SITES): {self.base_url}"
                )
                article_url = self._fetch_with_playwright(target_date)
            else:
                self.logger.info(f"è®¿é—®ç½‘ç«™: {self.base_url}")
                response = self._make_request(self.base_url)
                soup = BeautifulSoup(response.text, "html.parser")

                article_url = self._find_article_from_soup(soup, target_date)

                if not article_url and self.session.proxies.get("http"):
                    self.logger.warning(f"ä½¿ç”¨ä»£ç†æœªæ‰¾åˆ°æ–‡ç« ï¼Œå°è¯•ç¦ç”¨ä»£ç†ç›´æ¥è®¿é—®")
                    self.session.proxies = {"http": None, "https": None}
                    self.logger.info(f"è®¿é—®ç½‘ç«™: {self.base_url} (ç›´æ¥è¿æ¥)")
                    response = self._make_request(self.base_url)
                    soup = BeautifulSoup(response.text, "html.parser")
                    article_url = self._find_article_from_soup(soup, target_date)

                if not article_url:
                    self.logger.warning(f"{self.site_name}: ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–é‡è¯•")
                    article_url = self._fetch_with_playwright(target_date)

            if not article_url:
                self.logger.warning(f"{self.site_name}: æœªæ‰¾åˆ°æœ€æ–°æ–‡ç« ")
                return None

            return article_url

        except Exception as e:
            self.logger.error(f"{self.site_name}: è·å–æ–‡ç« URLå¤±è´¥ - {str(e)}")
            import traceback

            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def _find_article_from_soup(self, soup, target_date=None):
        """ä»BeautifulSoupå¯¹è±¡ä¸­æŸ¥æ‰¾æ–‡ç« URL - ä¼˜å…ˆä»Šå¤©ï¼Œå…¶æ¬¡æœ€è¿‘çš„"""
        # é»˜è®¤ä½¿ç”¨ä»Šå¤©ä½œä¸ºç›®æ ‡æ—¥æœŸ
        if target_date is None:
            target_date = datetime.now()

        # ç”Ÿæˆå¤šç§æ—¥æœŸæ ¼å¼ç”¨äºåŒ¹é…
        date_str = target_date.strftime("%Y-%m-%d")  # 2026-01-19
        date_str_no_padding = (
            f"{target_date.year}-{target_date.month}-{target_date.day}"  # 2026-1-19
        )
        date_str_alt = target_date.strftime("%Y/%m/%d")  # 2026/01/19
        date_str_alt_no_padding = (
            f"{target_date.year}/{target_date.month}/{target_date.day}"  # 2026/1/19
        )
        date_str_month_day_cn = f"{target_date.month}æœˆ{target_date.day}æ—¥"  # 1æœˆ19æ—¥
        date_str_month_day_cn_alt = (
            f"{target_date.month:02d}æœˆ{target_date.day:02d}æ—¥"  # 01æœˆ19æ—¥
        )
        date_str_month_day = target_date.strftime("%m-%d")  # 01-19
        date_str_month_day_no_padding = f"{target_date.month}-{target_date.day}"  # 1-19
        date_str_year_month = target_date.strftime("%Y-%m")  # 2026-01
        date_str_year_month_cn = f"{target_date.year}å¹´{target_date.month:02d}æœˆ{target_date.day:02d}æ—¥"  # 2026å¹´01æœˆ19æ—¥

        # æ”¶é›†æ‰€æœ‰åŒ…å«æ—¥æœŸçš„é“¾æ¥åŠå…¶æ—¥æœŸä¿¡æ¯
        dated_links = []
        all_links = soup.find_all("a", href=True)

        self.logger.debug(f"æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹æå–æ—¥æœŸ...")

        # ä¿å­˜HTMLå†…å®¹ç”¨äºè°ƒè¯•ï¼ˆé—®é¢˜ç½‘ç«™ï¼‰
        from src.config.websites import BROWSER_ONLY_SITES

        site_key = self.site_config.get("collector_key", self.site_config.get("name"))
        if site_key in BROWSER_ONLY_SITES:
            import os
            from datetime import datetime as dt

            debug_dir = os.path.join(os.getcwd(), "data", "debug")
            os.makedirs(debug_dir, exist_ok=True)
            debug_file = os.path.join(
                debug_dir,
                f"debug_{self.site_name}_{dt.now().strftime('%Y%m%d_%H%M%S')}.html",
            )
            try:
                html_content = str(soup)
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(html_content)
                self.logger.info(
                    f"ğŸ’¾ ä¿å­˜è°ƒè¯•HTMLåˆ°: {debug_file} ({len(html_content)} bytes)"
                )
            except Exception as e:
                self.logger.warning(f"ä¿å­˜è°ƒè¯•HTMLå¤±è´¥: {str(e)}")

        extracted_count = 0
        exclusion_reasons = {}  # ç»Ÿè®¡æ’é™¤åŸå› 

        for link in all_links:
            href = link.get("href")
            text = link.get_text(strip=True)
            title = link.get("title", "")  # æå–titleå±æ€§ï¼Œå¯èƒ½åŒ…å«æ—¥æœŸ

            if not href:
                continue

            # æ’é™¤å¯¼èˆªé“¾æ¥
            excluded = False
            exclusion_reason = ""
            if any(
                x in href
                for x in ["category", "tag", "page", "search", "about", "feed"]
            ):
                excluded = True
                exclusion_reason = "navigation link"
            elif href.startswith("#"):
                excluded = True
                exclusion_reason = "anchor link"
            elif len(href) < 10:
                excluded = True
                exclusion_reason = "href too short"

            if excluded:
                exclusion_reasons[exclusion_reason] = (
                    exclusion_reasons.get(exclusion_reason, 0) + 1
                )
                continue

            # å°è¯•ä»é“¾æ¥ã€æ–‡æœ¬æˆ–titleä¸­æå–æ—¥æœŸ
            link_date = self._extract_date_from_text(href, text, title)

            if link_date is not None:
                extracted_count += 1
                # è®¡ç®—ä¸ä»Šå¤©çš„å¤©æ•°å·®
                days_diff = abs((link_date.date() - target_date.date()).days)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä»Šå¤©çš„æ—¥æœŸ
                is_today = link_date.date() == target_date.date()

                dated_links.append(
                    {
                        "url": href,
                        "date": link_date,
                        "days_diff": days_diff,
                        "is_today": is_today,
                        "text": text,
                    }
                )

        self.logger.debug(
            f"æå–åˆ° {len(dated_links)} ä¸ªå¸¦æ—¥æœŸçš„é“¾æ¥ (å…±å°è¯•æå– {extracted_count} ä¸ª)"
        )

        # æ˜¾ç¤ºæ’é™¤ç»Ÿè®¡
        if exclusion_reasons:
            self.logger.debug(f"é“¾æ¥æ’é™¤ç»Ÿè®¡: {exclusion_reasons}")

        # æŒ‰æ—¥æœŸæ’åºï¼šä»Šå¤©çš„åœ¨å‰ï¼Œå…¶æ¬¡æŒ‰æ—¥æœŸæ–°æ—§
        dated_links.sort(key=lambda x: (not x["is_today"], x["days_diff"]))

        # å¦‚æœæœ‰ä»Šå¤©çš„æ—¥æœŸï¼Œè¿”å›ç¬¬ä¸€ä¸ª
        today_article = None
        latest_article = None

        for item in dated_links:
            if item["is_today"] and today_article is None:
                today_article = item
            if latest_article is None:
                latest_article = item

        if today_article:
            article_url = self._process_url(today_article["url"])
            self.logger.info(f"âœ… æ‰¾åˆ°ä»Šå¤©çš„æ–‡ç« : {article_url}")
            return article_url

        # å¦‚æœæ²¡æœ‰ä»Šå¤©çš„æ—¥æœŸï¼Œè¿”å›æœ€è¿‘çš„
        if latest_article:
            article_url = self._process_url(latest_article["url"])
            days_ago = (target_date.date() - latest_article["date"].date()).days
            if days_ago == 0:
                date_hint = "ä»Šå¤©"
            elif days_ago == 1:
                date_hint = "æ˜¨å¤©"
            else:
                date_hint = f"{days_ago}å¤©å‰"

            self.logger.info(
                f"âš ï¸ æœªæ‰¾åˆ°ä»Šå¤©çš„æ–‡ç« ï¼Œä½¿ç”¨æœ€æ–°çš„ ({date_hint} - {latest_article['date'].strftime('%Y-%m-%d')}): {article_url}"
            )
            return article_url

        # å¦‚æœæ—¥æœŸåŒ¹é…å¤±è´¥ï¼Œå°è¯•ç‰¹å®šé€‰æ‹©å™¨
        selectors = self.site_config.get("selectors", [])
        for selector in selectors:
            links = soup.select(selector)
            if links:
                href = links[0].get("href")
                if href:
                    article_url = self._process_url(href)
                    self.logger.info(f"é€šè¿‡é€‰æ‹©å™¨æ‰¾åˆ°æ–‡ç« : {article_url}")
                    return article_url

        # å°è¯•é€šç”¨é€‰æ‹©å™¨
        for selector in UNIVERSAL_SELECTORS:
            links = soup.select(selector)
            if links:
                href = links[0].get("href")
                if href:
                    article_url = self._process_url(href)
                    self.logger.info(f"é€šè¿‡é€šç”¨é€‰æ‹©å™¨æ‰¾åˆ°æ–‡ç« : {article_url}")
                    return article_url

        # å°è¯•æŸ¥æ‰¾ä»Šæ—¥é“¾æ¥
        today_url = self._find_today_article(soup)
        if today_url:
            return today_url

        # å°è¯•é€šè¿‡æ—¶é—´æŸ¥æ‰¾
        time_url = self._find_by_time(soup)
        if time_url:
            return time_url

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæ‰æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        self.logger.warning(f"æœªæ‰¾åˆ°å¸¦æ—¥æœŸçš„é“¾æ¥ï¼Œæ˜¾ç¤ºå‰3ä¸ªé“¾æ¥æ ·ä¾‹:")
        sample_links = all_links[:3]
        for i, link in enumerate(sample_links):
            href = link.get("href", "")
            text = link.get_text(strip=True)[:50]
            self.logger.warning(f"  [{i + 1}] {href[:80]}... (æ–‡æœ¬: {text})")
        
        self.logger.warning(f"æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥")
        return None

    def _extract_date_from_text(self, href, text, title=""):
        """ä»é“¾æ¥URLã€æ–‡æœ¬æˆ–titleå±æ€§ä¸­æå–æ—¥æœŸ"""
        import re
        from datetime import datetime

        # å¸¸è§æ—¥æœŸæ ¼å¼æ¨¡å¼
        date_patterns = [
            # URLä¸­çš„æ—¥æœŸæ ¼å¼
            r"/(\d{4})-(\d{1,2})-(\d{1,2})/",  # /2026-1-19/ æˆ– /2026-01-19/
            r"/(\d{4})/(\d{1,2})/(\d{1,2})/",  # /2026/1/19/ æˆ– /2026/01/19/
            r"/(\d{4})-(\d{1,2})-(\d{1,2})\.",  # /2026-1-19. æˆ– /2026-01-19.
            r"/(\d{4})/(\d{1,2})/(\d{1,2})\.",  # /2026/1/19. æˆ– /2026/01/19.
            # æ–‡æœ¬ä¸­çš„æ—¥æœŸæ ¼å¼ - ä¸­æ–‡æ ¼å¼
            r"(\d{1,2})æœˆ(\d{1,2})æ—¥",  # 1æœˆ19æ—¥ æˆ– 01æœˆ19æ—¥
            r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",  # 2026å¹´1æœˆ19æ—¥
            r"(\d{2})-(\d{1,2})-(\d{1,2})",  # 26-01-19 (å‡è®¾21ä¸–çºª)
            r"(\d{2})\.(\d{1,2})\.(\d{1,2})",  # 26.01.19 (å‡è®¾21ä¸–çºª)
        ]

        # åˆå¹¶æ‰€æœ‰å¯ç”¨æ–‡æœ¬
        combined_text = f"{href} {text} {title}"

        today = datetime.now()

        for pattern in date_patterns:
            match = re.search(pattern, combined_text)
            if match:
                try:
                    groups = match.groups()
                    groups_len = len(groups)

                    year = None
                    month = None
                    day = None

                    # æ ¹æ®æ¨¡å¼å’Œç»„æ•°å¤„ç†
                    if groups_len == 3:
                        # 3ä¸ªç»„ï¼šå¯èƒ½æ˜¯ URL æ ¼å¼æˆ– "å¹´/æœˆ/æ—¥" ä¸­æ–‡æ ¼å¼
                        if "æœˆ" in pattern and "å¹´" in pattern:
                            # 2026å¹´1æœˆ19æ—¥ æ ¼å¼
                            year, month, day = (
                                int(groups[0]),
                                int(groups[1]),
                                int(groups[2]),
                            )
                        else:
                            # URL æ ¼å¼: 2026-1-19 æˆ– 26-01-19
                            year = int(groups[0])
                            month = int(groups[1])
                            day = int(groups[2])

                            # å¤„ç†ä¸¤ä½æ•°å¹´ä»½
                            if year < 100:
                                year = 2000 + year

                    elif groups_len == 2:
                        # 2ä¸ªç»„ï¼šåªæœ‰æœˆæ—¥çš„ä¸­æ–‡æ ¼å¼ (å¦‚ 1æœˆ18æ—¥)
                        if "æœˆ" in pattern:
                            year = today.year
                            month = int(groups[0])
                            day = int(groups[1])
                        else:
                            # å…¶ä»–2ç»„æ ¼å¼ï¼Œä¸å¤„ç†
                            continue

                    else:
                        # ä¸æ”¯æŒçš„ç»„æ•°
                        continue

                    # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                    if (
                        year
                        and month
                        and day
                        and 2020 <= year <= 2030
                        and 1 <= month <= 12
                        and 1 <= day <= 31
                    ):
                        return datetime(year, month, day)

                except (ValueError, TypeError):
                    continue

        return None

    def _fetch_with_playwright(self, target_date=None):
        """ä½¿ç”¨Playwrightæµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–é¡µé¢å†…å®¹ï¼ˆç¦ç”¨ä»£ç†ï¼‰"""
        try:
            self.logger.info(f"å¯åŠ¨æµè§ˆå™¨è®¿é—®: {self.base_url} (ç¦ç”¨ä»£ç†)")

            # ä¸´æ—¶ç¦ç”¨ä»£ç†
            original_proxies = self.session.proxies
            self.session.proxies = {"http": None, "https": None}
            self.logger.debug("ä¸´æ—¶ç¦ç”¨ä»£ç†")

            # ä½¿ç”¨æµè§ˆå™¨è·å–å†…å®¹
            content = self._fetch_page_content_with_browser()

            # æ¢å¤ä»£ç†è®¾ç½®
            self.session.proxies = original_proxies
            self.logger.debug("æ¢å¤ä»£ç†è®¾ç½®")

            self.logger.info(f"æµè§ˆå™¨è·å–åˆ° {len(content)} å­—èŠ‚å†…å®¹")

            # ä¿å­˜è°ƒè¯•HTML
            self._save_debug_html(content)

            # è§£ææ–‡ç« URL
            soup = BeautifulSoup(content, "html.parser")
            article_url = self._find_article_from_soup(soup, target_date)

            if article_url:
                self.logger.info(f"âœ… æ‰¾åˆ°æ–‡ç« URL: {article_url}")
            else:
                self.logger.warning(f"âŒ æœªæ‰¾åˆ°æ–‡ç« URL")

            return article_url

        except Exception as e:
            self.logger.error(f"Playwrightè®¿é—®å¤±è´¥: {str(e)}")
            # æ¢å¤ä»£ç†è®¾ç½®ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè¦æ¢å¤ï¼‰
            if "original_proxies" in dir():
                self.session.proxies = original_proxies
            return None

    def _fetch_page_content_with_browser(self) -> str:
        """
        ä½¿ç”¨æµè§ˆå™¨è·å–é¡µé¢å†…å®¹

        Returns:
            é¡µé¢HTMLå†…å®¹
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True, args=["--no-sandbox", "--disable-dev-shm-usage"]
            )
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                locale="zh-CN",
            )
            page = context.new_page()
            # ä½¿ç”¨ domcontentloaded é¿å… networkidle è¶…æ—¶
            page.goto(self.base_url, wait_until="domcontentloaded", timeout=30000)
            # ç­‰å¾…é¢å¤–æ—¶é—´è®©JSæ‰§è¡Œ
            page.wait_for_timeout(3000)
            content = page.content()
            browser.close()

        return content

    def _save_debug_html(self, content: str):
        """
        ä¿å­˜è°ƒè¯•HTMLåˆ°æ–‡ä»¶

        Args:
            content: HTMLå†…å®¹
        """
        import os
        from datetime import datetime as dt

        debug_dir = os.path.join(os.getcwd(), "data", "debug")
        os.makedirs(debug_dir, exist_ok=True)
        debug_file = os.path.join(
            debug_dir,
            f"debug_{self.site_name}_{dt.now().strftime('%Y%m%d_%H%M%S')}.html",
        )
        try:
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(content)
            self.logger.info(f"ğŸ’¾ ä¿å­˜è°ƒè¯•HTMLåˆ°: {debug_file} ({len(content)} bytes)")
        except Exception as e:
            self.logger.warning(f"ä¿å­˜è°ƒè¯•HTMLå¤±è´¥: {str(e)}")

    def extract_nodes_from_article(self, article_url):
        """ä»æ–‡ç« ä¸­æå–èŠ‚ç‚¹"""
        try:
            self.logger.info(f"è§£ææ–‡ç« : {article_url}")
            response = self._make_request(article_url)

            content = response.text
            self.raw_data = content

            # æŸ¥æ‰¾è®¢é˜…é“¾æ¥
            subscription_links = self.find_subscription_links(content)
            self.subscription_links = subscription_links

            nodes = []

            # ä»è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹
            for link in subscription_links:
                self.logger.info(f"æ‰¾åˆ°è®¢é˜…é“¾æ¥: {link}")
                sub_nodes = self.get_nodes_from_subscription(link)
                nodes.extend(sub_nodes)
                time.sleep(self.delay)  # é¿å…è¯·æ±‚è¿‡å¿«

            # ç›´æ¥ä»é¡µé¢æå–èŠ‚ç‚¹
            direct_nodes = self.extract_direct_nodes(content)
            nodes.extend(direct_nodes)

            # å»é‡
            nodes = list(set(nodes))

            return nodes

        except Exception as e:
            self.logger.error(f"è§£ææ–‡ç« å¤±è´¥: {str(e)}")
            return []

    def find_subscription_links(self, content):
        """æŸ¥æ‰¾è®¢é˜…é“¾æ¥"""
        links = []

        # ä½¿ç”¨ç‰¹å®šç½‘ç«™çš„æ¨¡å¼
        patterns = self.site_config.get("patterns", [])
        for pattern in patterns:
            try:
                matches = re.findall(pattern, content, re.IGNORECASE)
                links.extend(matches)
            except Exception as e:
                self.logger.warning(f"æ¨¡å¼åŒ¹é…å¤±è´¥: {pattern} - {str(e)}")

        # ä½¿ç”¨é€šç”¨è®¢é˜…æ¨¡å¼
        for pattern in SUBSCRIPTION_PATTERNS:
            try:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        links.extend(match)
                    else:
                        links.append(match)
            except Exception as e:
                self.logger.warning(f"é€šç”¨æ¨¡å¼åŒ¹é…å¤±è´¥: {pattern} - {str(e)}")

        # åœ¨å…³é”®è¯é™„è¿‘æŸ¥æ‰¾
        for keyword in SUBSCRIPTION_KEYWORDS:
            try:
                pattern = rf"{keyword}[^:]*[:ï¼š]\s*(https?://[^\s\n\r]+)"
                matches = re.findall(pattern, content, re.IGNORECASE)
                links.extend(matches)
            except Exception:
                self.logger.debug(f"SUBSCRIPTION_KEYWORDSæ¨¡å¼åŒ¹é…å¤±è´¥: {keyword}")

        # æ¸…ç†å’Œå»é‡
        cleaned_links = []
        seen = set()

        for link in links:
            # å…ˆä»åŸå§‹é“¾æ¥ä¸­æå–æ‰€æœ‰ç‹¬ç«‹çš„.txt URLï¼ˆé¿å…å…ˆæ¸…ç†å¯¼è‡´URLåˆå¹¶ï¼‰
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–æ‰€æœ‰URLï¼Œåœ¨é‡åˆ°HTMLæ ‡ç­¾æ—¶åœæ­¢
            url_matches = re.findall(r'https?://[^<\s"]+(?:\.(?:txt|TXT))', link)

            for url_match in url_matches:
                # ç„¶åå¯¹æ¯ä¸ªæå–çš„URLè¿›è¡Œæ¸…ç†
                clean_link = self._clean_link(url_match)
                if (
                    clean_link
                    and clean_link not in seen
                    and self._is_valid_url(clean_link)
                    and self._is_valid_subscription_link(clean_link)
                ):
                    cleaned_links.append(clean_link)
                    seen.add(clean_link)

        return cleaned_links

    def get_nodes_from_subscription(self, subscription_url):
        """ä»è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹ - æ”¯æŒå¤šç§ç¼–ç æ ¼å¼"""
        try:
            self.logger.info(f"è·å–è®¢é˜…å†…å®¹: {subscription_url}")
            response = self._make_request(subscription_url)

            content = response.text.strip()

            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not content:
                self.logger.warning(f"è®¢é˜…é“¾æ¥è¿”å›ç©ºå†…å®¹: {subscription_url}")
                return []

            if len(content) < 10:  # å¤ªçŸ­ä¸å¯èƒ½æ˜¯æœ‰æ•ˆèŠ‚ç‚¹
                self.logger.warning(
                    f"è®¢é˜…é“¾æ¥å†…å®¹è¿‡çŸ­ ({len(content)} å­—ç¬¦): {subscription_url}"
                )
                return []

            # å°è¯•å¤šç§è§£ææ–¹å¼
            all_nodes = []

            # æ–¹å¼1: ç›´æ¥ä»åŸå§‹å†…å®¹æå–
            nodes = self._extract_nodes_from_text(content)
            if nodes:
                self.logger.info(f"ç›´æ¥è§£æè·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
                all_nodes.extend(nodes)

            # æ–¹å¼2: å°è¯•Base64è§£ç 
            all_nodes.extend(self._try_base64_decode(content))

            # æ–¹å¼3: å°è¯•URLè§£ç 
            all_nodes.extend(self._try_url_decode(content))

            # æ–¹å¼4: é€è¡Œåˆ†å‰²åæå–ï¼ˆå¤„ç†æŸäº›ç‰¹æ®Šæ ¼å¼ï¼‰
            all_nodes.extend(self._try_line_by_line_parse(content))

            # æ–¹å¼5: å°è¯•è§£æ YAML/JSON æ ¼å¼ï¼ˆClashé…ç½®ï¼‰
            yaml_nodes = self._extract_yaml_json_nodes(content)
            if yaml_nodes:
                self.logger.info(f"YAML/JSONæ ¼å¼è§£æè·å–åˆ° {len(yaml_nodes)} ä¸ªèŠ‚ç‚¹")
                all_nodes.extend(yaml_nodes)

            # å»é‡å’Œè¿‡æ»¤
            unique_nodes = self._deduplicate_and_filter_nodes(all_nodes)

            self.logger.info(
                f"ä»è®¢é˜…é“¾æ¥è·å–åˆ° {len(unique_nodes)} ä¸ªèŠ‚ç‚¹ (åŸå§‹: {len(all_nodes)})"
            )
            return unique_nodes

        except Exception as e:
            self.logger.error(f"è·å–è®¢é˜…é“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _try_base64_decode(self, content: str) -> List[str]:
        """
        å°è¯•Base64è§£ç å†…å®¹

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            è§£æåˆ°çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        nodes = []
        try:
            import base64

            # è¡¥é½base64 padding
            padded_content = content + "=" * (-len(content) % 4)
            decoded_content = base64.b64decode(padded_content).decode(
                "utf-8", errors="ignore"
            )
            nodes = self._extract_nodes_from_text(decoded_content)
            if nodes:
                self.logger.info(f"Base64è§£ç åè·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
                return nodes

            # å°è¯•åŒé‡Base64è§£ç ï¼ˆæŸäº›è®¢é˜…é“¾æ¥ä½¿ç”¨åŒé‡ç¼–ç ï¼‰
            decoded_bytes = base64.b64decode(padded_content)
            double_padded = decoded_bytes + b"=" * (-len(decoded_bytes) % 4)
            double_decoded = base64.b64decode(double_padded).decode(
                "utf-8", errors="ignore"
            )
            nodes = self._extract_nodes_from_text(double_decoded)
            if nodes:
                self.logger.info(f"åŒé‡Base64è§£ç åè·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")

        except Exception as e:
            self.logger.debug(f"Base64è§£ç å¤±è´¥: {str(e)}")

        return nodes

    def _try_url_decode(self, content: str) -> List[str]:
        """
        å°è¯•URLè§£ç å†…å®¹

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            è§£æåˆ°çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        nodes = []
        try:
            from urllib.parse import unquote

            url_decoded = unquote(content)
            if url_decoded != content:  # ç¡®å®å‘ç”Ÿäº†è§£ç 
                nodes = self._extract_nodes_from_text(url_decoded)
                if nodes:
                    self.logger.info(f"URLè§£ç åè·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
        except Exception as e:
            self.logger.debug(f"URLè§£ç å¤±è´¥: {str(e)}")

        return nodes

    def _try_line_by_line_parse(self, content: str) -> List[str]:
        """
        é€è¡Œè§£æå†…å®¹ï¼ˆå¤„ç†æŸäº›ç‰¹æ®Šæ ¼å¼ï¼‰

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            è§£æåˆ°çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        nodes = []
        lines = content.split("\n")

        for line in lines:
            line = line.strip()
            if not line or len(line) < 10:
                continue

            # å°è¯•ä»å•è¡Œæå–èŠ‚ç‚¹
            nodes.extend(self._extract_nodes_from_text(line))

            # å°è¯•Base64è§£ç å•è¡Œ
            try:
                import base64

                padded_line = line + "=" * (-len(line) % 4)
                decoded_line = base64.b64decode(padded_line).decode(
                    "utf-8", errors="ignore"
                )
                nodes.extend(self._extract_nodes_from_text(decoded_line))

                # å°è¯•åŒé‡Base64è§£ç 
                decoded_bytes = base64.b64decode(padded_line)
                double_padded = decoded_bytes + b"=" * (-len(decoded_bytes) % 4)
                double_decoded = base64.b64decode(double_padded).decode(
                    "utf-8", errors="ignore"
                )
                nodes.extend(self._extract_nodes_from_text(double_decoded))
            except Exception:
                pass

        return nodes

    def _deduplicate_and_filter_nodes(self, nodes: List[str]) -> List[str]:
        """
        å»é‡å’Œè¿‡æ»¤èŠ‚ç‚¹

        Args:
            nodes: åŸå§‹èŠ‚ç‚¹åˆ—è¡¨

        Returns:
            å»é‡å’Œè¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # å»é‡
        unique_nodes = list(set(nodes))

        # è¿‡æ»¤é•¿åº¦
        unique_nodes = [
            node for node in unique_nodes if len(node) >= MIN_NODE_LENGTH
        ]

        return unique_nodes

    def _extract_nodes_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­æå–èŠ‚ç‚¹"""
        nodes = []
        for pattern in NODE_PATTERNS:
            try:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    node = match.strip()
                    if node and len(node) >= MIN_NODE_LENGTH:
                        nodes.append(node)
            except Exception as e:
                self.logger.warning(f"èŠ‚ç‚¹æ¨¡å¼åŒ¹é…å¤±è´¥: {pattern} - {str(e)}")
        return nodes

    def _extract_yaml_json_nodes(self, content):
        """ä» YAML/JSON æ ¼å¼æå–èŠ‚ç‚¹ï¼ˆClashé…ç½®æ ¼å¼ï¼‰"""
        nodes = []
        try:
            # é¦–å…ˆå°è¯•ä½œä¸ºå®Œæ•´JSONè§£æ
            json_nodes = self._parse_json_format(content)
            if json_nodes:
                return json_nodes

            # å°è¯•ä½œä¸ºYAMLè§£æ
            yaml_nodes = self._parse_yaml_format(content)
            if yaml_nodes:
                return yaml_nodes

            # å›é€€æ–¹æ¡ˆï¼šä»æ–‡æœ¬ä¸­æå–è¡Œå†…JSONå¯¹è±¡
            inline_json_nodes = self._parse_inline_json(content)
            if inline_json_nodes:
                return inline_json_nodes

        except ImportError as e:
            self.logger.warning(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
        except Exception as e:
            self.logger.warning(f"YAML/JSON è§£æå¤±è´¥: {str(e)}")

        return nodes

    def _parse_json_format(self, content: str) -> List[str]:
        """è§£æJSONæ ¼å¼å†…å®¹"""
        nodes = []
        try:
            import json

            data = json.loads(content.strip())
            proxies_list = None

            if isinstance(data, list):
                proxies_list = data
                self.logger.info(f"è¯†åˆ«ä¸ºJSONæ•°ç»„æ ¼å¼ï¼ŒåŒ…å« {len(proxies_list)} ä¸ªä»£ç†")
            elif isinstance(data, dict) and "proxies" in data:
                proxies_list = data["proxies"]
                self.logger.info(f"è¯†åˆ«ä¸ºJSONå¯¹è±¡æ ¼å¼ï¼ŒåŒ…å« {len(proxies_list)} ä¸ªä»£ç†")

            if proxies_list:
                for proxy in proxies_list:
                    try:
                        node = self._convert_clash_proxy_to_node(proxy)
                        if node and len(node) >= MIN_NODE_LENGTH:
                            nodes.append(node)
                    except Exception as e:
                        self.logger.debug(f"JSONä»£ç†è½¬æ¢å¤±è´¥: {str(e)}")

        except json.JSONDecodeError:
            pass

        return nodes

    def _parse_yaml_format(self, content: str) -> List[str]:
        """è§£æYAMLæ ¼å¼å†…å®¹"""
        nodes = []
        try:
            import yaml

            self.logger.info("å¼€å§‹å°è¯•YAMLè§£æ...")
            yaml_data = yaml.safe_load(content.strip())
            self.logger.info(f"YAMLè§£ææˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(yaml_data)}")

            if isinstance(yaml_data, dict) and "proxies" in yaml_data:
                return self._process_yaml_proxies(yaml_data["proxies"], "Clashé…ç½®æ ¼å¼")

            elif isinstance(yaml_data, list):
                return self._process_yaml_proxies(yaml_data, "ä»£ç†åˆ—è¡¨æ ¼å¼")

            else:
                self.logger.info(f"YAMLæ•°æ®æ ¼å¼ä¸æ”¯æŒ: {type(yaml_data)}")

        except Exception as e:
            pass

        return nodes

    def _process_yaml_proxies(self, proxies_list: List, format_name: str) -> List[str]:
        """å¤„ç†YAMLä»£ç†åˆ—è¡¨"""
        nodes = []
        self.logger.info(f"è¯†åˆ«ä¸º{format_name}ï¼ŒåŒ…å« {len(proxies_list)} ä¸ªä»£ç†")

        for i, proxy in enumerate(proxies_list):
            try:
                node = self._convert_clash_proxy_to_node(proxy)
                if node and len(node) >= MIN_NODE_LENGTH:
                    nodes.append(node)
                else:
                    self.logger.warning(f"YAMLä»£ç† {i + 1} è½¬æ¢ç»“æœå¤ªçŸ­æˆ–ä¸ºç©º")
            except Exception as e:
                self.logger.warning(f"YAMLä»£ç† {i + 1} è½¬æ¢å¤±è´¥: {str(e)}")

        self.logger.info(f"YAMLè§£æå®Œæˆï¼Œå…±è½¬æ¢ {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes

    def _parse_inline_json(self, content: str) -> List[str]:
        """ä»YAMLæ–‡æœ¬ä¸­æå–è¡Œå†…JSONå¯¹è±¡"""
        nodes = []

        if "proxies:" not in content and "proxies" not in content:
            return nodes

        try:
            import json

            lines = content.split("\n")
            json_objects = self._extract_json_objects_from_lines(lines)

            self.logger.info(f"ä»YAMLæ–‡æœ¬ä¸­æ‰¾åˆ° {len(json_objects)} ä¸ªè¡Œå†…JSONå¯¹è±¡")

            success_count = 0
            error_count = 0

            for json_str in json_objects:
                try:
                    proxy = json.loads(json_str)
                    node = self._convert_clash_proxy_to_node(proxy)
                    if node and len(node) >= MIN_NODE_LENGTH:
                        nodes.append(node)
                        success_count += 1
                    else:
                        error_count += 1
                except json.JSONDecodeError as e:
                    error_count += 1
                    if error_count <= 3:
                        self.logger.warning(f"è¡Œå†…JSONè§£æå¤±è´¥: {str(e)[:100]}")
                except Exception as e:
                    error_count += 1

            self.logger.info(f"YAMLè¡Œå†…JSONè§£æå®Œæˆ: {success_count} æˆåŠŸ, {error_count} å¤±è´¥")

        except Exception as e:
            self.logger.warning(f"è¡Œå†…JSONè§£æå¤±è´¥: {str(e)}")

        return nodes

    def _extract_json_objects_from_lines(self, lines: List[str]) -> List[str]:
        """ä»è¡Œåˆ—è¡¨ä¸­æå–JSONå¯¹è±¡"""
        json_objects = []

        for line in lines:
            line = line.strip()
            # æŸ¥æ‰¾è¡Œå†…çš„ JSON å¯¹è±¡ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
            json_matches = re.findall(r"-\s*(\{[^}]*\{[^}]*\}[^}]*\})", line)
            if not json_matches:
                json_match = re.search(r"-\s*(\{.+\})", line)
                if json_match:
                    json_matches.append(json_match.group(1))

            for json_str in json_matches:
                json_objects.append(json_str)

        return json_objects

    def _convert_clash_proxy_to_node(self, proxy):
        """å°† Clash proxy å¯¹è±¡è½¬æ¢ä¸º V2Ray èŠ‚ç‚¹ URI æ ¼å¼"""
        return self.converter.convert(proxy)

    def get_v2ray_subscription_links(self, article_url):
        """è·å–V2Rayè®¢é˜…é“¾æ¥"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°ï¼ˆç›´æ¥è®¢é˜…é“¾æ¥ï¼‰
            if article_url.endswith("#direct_subscription"):
                subscription_url = article_url.replace("#direct_subscription", "")
                self.logger.info(f"æ£€æµ‹åˆ°ç›´æ¥è®¢é˜…é“¾æ¥: {subscription_url}")
                return [subscription_url]

            self.logger.info(f"è§£ææ–‡ç« : {article_url}")
            response = self._make_request(article_url)

            content = response.text

            # æŸ¥æ‰¾æ‰€æœ‰è®¢é˜…é“¾æ¥
            subscription_links = self.find_subscription_links(content)

            # è¿‡æ»¤å‡ºV2Rayè®¢é˜…é“¾æ¥
            v2ray_links = []
            for link in subscription_links:
                if self.is_v2ray_subscription(link):
                    v2ray_links.append(link)

            self.logger.info(f"ä»æ–‡ç« ä¸­æ‰¾åˆ° {len(v2ray_links)} ä¸ªV2Rayè®¢é˜…é“¾æ¥")
            return v2ray_links

        except Exception as e:
            self.logger.error(f"è·å–V2Rayè®¢é˜…é“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def is_v2ray_subscription(self, url):
        """åˆ¤æ–­æ˜¯å¦ä¸ºV2Rayè®¢é˜…é“¾æ¥"""
        try:
            # è®¢é˜…é“¾æ¥é€šå¸¸ä»¥httpå¼€å¤´
            if not url.startswith("http"):
                return False

            from urllib.parse import urlparse

            parsed = urlparse(url)
            path_part = parsed.path.lower()
            domain = parsed.netloc.lower()

            # å¿…é¡»ä»¥.txtã€.yamlã€.jsonç»“å°¾ï¼ˆV2Rayæ ¼å¼ï¼‰
            if not path_part.endswith((".txt", ".yaml", ".json")):
                return False

            # æ’é™¤æ˜æ˜¾çš„Clashå’ŒSing-Boxè®¢é˜…é“¾æ¥
            excluded_keywords = [
                "sing-box",
                "singbox",
                "yaml",
                "yml",
                "clashå…è´¹èŠ‚ç‚¹",
                "sing-boxå…è´¹èŠ‚ç‚¹",
                "Clashå…è´¹èŠ‚ç‚¹",
                "Sing-Boxå…è´¹èŠ‚ç‚¹",
            ]

            # åªæ£€æŸ¥è·¯å¾„éƒ¨åˆ†ï¼Œä¸æ£€æŸ¥åŸŸå
            for keyword in excluded_keywords:
                if keyword in path_part:
                    return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«V2Rayç›¸å…³å…³é”®è¯
            v2ray_keywords = ["v2ray", "sub", "subscribe", "node", "link"]
            for keyword in v2ray_keywords:
                if keyword in path_part:
                    return True

            return True

        except Exception:
            return False

    def extract_direct_nodes(self, content):
        """ç›´æ¥ä»é¡µé¢å†…å®¹æå–èŠ‚ç‚¹"""
        nodes = []

        # ä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹æ¨¡å¼
        for pattern in NODE_PATTERNS:
            try:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    node = match.strip()
                    if len(node) >= MIN_NODE_LENGTH:
                        nodes.append(node)
            except Exception as e:
                self.logger.warning(f"èŠ‚ç‚¹æ¨¡å¼åŒ¹é…å¤±è´¥: {pattern} - {str(e)}")

        # ä»ä»£ç å—ä¸­æå–
        for selector in CODE_BLOCK_SELECTORS:
            try:
                matches = re.findall(selector, content, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    block_nodes = self.parse_node_text(match)
                    nodes.extend(block_nodes)
            except Exception as e:
                self.logger.warning(f"ä»£ç å—åŒ¹é…å¤±è´¥: {selector} - {str(e)}")

        # ä»Base64å†…å®¹ä¸­æå–
        for pattern in BASE64_PATTERNS:
            try:
                matches = re.findall(pattern, content)
                for match in matches:
                    try:
                        import base64

                        decoded = base64.b64decode(match).decode(
                            "utf-8", errors="ignore"
                        )
                        if any(
                            proto in decoded.lower() for proto in SUPPORTED_PROTOCOLS
                        ):
                            decoded_nodes = self.parse_node_text(decoded)
                            nodes.extend(decoded_nodes)
                    except Exception as e:
                        self.logger.debug(f"å•è¡ŒBase64è§£ç å¤±è´¥: {str(e)}")
            except Exception as e:
                self.logger.debug(f"é€è¡Œå¤„ç†å¤±è´¥: {str(e)}")

        return list(set(nodes))  # å»é‡

    def parse_node_text(self, text):
        """è§£ææ–‡æœ¬ä¸­çš„èŠ‚ç‚¹ä¿¡æ¯"""
        nodes = []

        for pattern in NODE_PATTERNS:
            try:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    node = match.strip()
                    if node and len(node) >= MIN_NODE_LENGTH:
                        nodes.append(node)
            except Exception as e:
                self.logger.warning(f"èŠ‚ç‚¹è§£æå¤±è´¥: {pattern} - {str(e)}")

        # ä¿®å¤è¢«é”™è¯¯æ ‡è®°ä¸ºss://çš„VMessèŠ‚ç‚¹
        nodes = self._fix_misidentified_nodes(nodes)

        return list(set(nodes))  # å»é‡

    def _fix_misidentified_nodes(self, nodes):
        """ä¿®å¤è¢«é”™è¯¯æ ‡è®°ä¸ºss://çš„VMessèŠ‚ç‚¹"""
        fixed_nodes = []

        for node in nodes:
            if node.startswith("ss://"):
                # å°è¯•è§£ç å¹¶æ£€æŸ¥æ˜¯å¦ä¸ºVMessèŠ‚ç‚¹
                try:
                    import base64
                    import json

                    # å»æ‰ ss://
                    data = node[5:]
                    # è¡¥é½base64
                    data += "=" * (-len(data) % 4)
                    # URLè§£ç 
                    try:
                        from urllib.parse import unquote

                        data = unquote(data)
                    except Exception as e:
                        self.logger.debug(f"URLè§£ç å¤±è´¥: {str(e)}")

                    decoded = base64.b64decode(data).decode("utf-8", errors="ignore")

                    # æ£€æŸ¥æ˜¯å¦ä¸ºVMess JSONæ ¼å¼
                    if decoded.startswith("{") and decoded.endswith("}"):
                        try:
                            config = json.loads(decoded)
                            if config.get("v") == "2":
                                # è¿™æ˜¯VMessèŠ‚ç‚¹ï¼Œè½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                                self.logger.debug(
                                    f"æ£€æµ‹åˆ°è¢«é”™è¯¯æ ‡è®°ä¸ºss://çš„VMessèŠ‚ç‚¹ï¼Œå·²ä¿®å¤"
                                )
                                fixed_nodes.append(
                                    f"vmess://{base64.b64encode(decoded.encode()).decode()}"
                                )
                                continue
                        except Exception as e:
                            self.logger.debug(f"VMess JSONè§£æå¤±è´¥: {str(e)}")
                except Exception as e:
                    self.logger.debug(f"ssèŠ‚ç‚¹ä¿®å¤å¤±è´¥: {str(e)}")

            # ä¿æŒåŸæ ·
            fixed_nodes.append(node)

        return fixed_nodes

    def save_raw_data(self, article_url):
        """ä¿å­˜åŸå§‹æ•°æ®"""
        try:
            if not os.path.exists(RAW_DATA_DIR):
                os.makedirs(RAW_DATA_DIR)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.site_name}_{timestamp}.html"
            filepath = os.path.join(RAW_DATA_DIR, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(f"<!-- URL: {article_url} -->\n")
                f.write(f"<!-- Time: {datetime.now().isoformat()} -->\n")
                f.write(self.raw_data)

            self.logger.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜: {filepath}")

        except Exception as e:
            self.logger.error(f"ä¿å­˜åŸå§‹æ•°æ®å¤±è´¥: {str(e)}")

    def _process_url(self, url):
        """å¤„ç†URL"""
        if url.startswith("/"):
            return urljoin(self.base_url, url)
        return url

    def _find_today_article(self, soup):
        """æŸ¥æ‰¾ä»Šæ—¥æ–‡ç« """
        try:
            today = datetime.now()
            date_patterns = [
                f"{today.month}-{today.day}",
                f"{today.year}-{today.month}-{today.day}",
                f"{today.month}æœˆ{today.day}æ—¥",
                f"{today.year}å¹´{today.month}æœˆ{today.day}æ—¥",
            ]

            all_links = soup.find_all("a", href=True)

            for link in all_links:
                href = link.get("href")
                text = link.get_text().strip()

                for pattern in date_patterns:
                    if pattern in text:
                        article_url = self._process_url(href)
                        self.logger.info(f"æ‰¾åˆ°ä»Šæ—¥æ–‡ç« : {article_url}")
                        return article_url

            return None

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ä»Šæ—¥æ–‡ç« å¤±è´¥: {str(e)}")
            return None

    def _find_by_time(self, soup):
        """é€šè¿‡æ—¶é—´æŸ¥æ‰¾æ–‡ç« """
        try:
            for time_selector in TIME_SELECTORS:
                time_elements = soup.select(time_selector)
                if time_elements:
                    for time_elem in time_elements:
                        parent = time_elem.find_parent(["article", ".post", ".entry"])
                        if parent:
                            article_link = parent.find("a")
                            if article_link and article_link.get("href"):
                                article_url = self._process_url(
                                    article_link.get("href")
                                )
                                self.logger.info(f"é€šè¿‡æ—¶é—´æ‰¾åˆ°æ–‡ç« : {article_url}")
                                return article_url

            return None

        except Exception as e:
            self.logger.error(f"é€šè¿‡æ—¶é—´æŸ¥æ‰¾æ–‡ç« å¤±è´¥: {str(e)}")
            return None

    def _clean_link(self, link):
        """æ¸…ç†é“¾æ¥ - ç°åœ¨æ¥æ”¶çš„å·²ç»æ˜¯ç‹¬ç«‹çš„URL"""
        if not link:
            return ""

        clean_link = link.strip()

        # ç§»é™¤HTMLå®ä½“ç¼–ç 
        clean_link = (
            clean_link.replace("%3C", "").replace("%3E", "").replace("%20", " ")
        )
        clean_link = (
            clean_link.replace("&lt;", "").replace("&gt;", "").replace("&nbsp;", " ")
        )

        # å¤„ç†å¸¸è§çš„é”™è¯¯é“¾æ¥æ ¼å¼ï¼ˆåªåœ¨æœ«å°¾åŒ¹é…ï¼‰
        for suffix in ["/strong", "/span", "/pp", "/h3", "&nbsp;", "/div", "div", "/p"]:
            if clean_link.endswith(suffix):
                clean_link = clean_link[: -len(suffix)]

        # ç§»é™¤æœ«å°¾çš„HTMLæ ‡ç­¾ï¼ˆåŒ…æ‹¬å®Œæ•´æ ‡ç­¾å¦‚</p>ã€</div>ç­‰ï¼‰
        clean_link = re.sub(r"<[^>]+>$", "", clean_link)
        # ç§»é™¤æœ«å°¾çš„æ— æ•ˆå­—ç¬¦
        clean_link = re.sub(r'[<>"]+$', "", clean_link)
        clean_link = clean_link.rstrip(";/")

        # ç¡®ä¿åªè¿”å›æœ‰æ•ˆçš„URL
        if not clean_link.startswith("http"):
            return ""

        # éªŒè¯URLæ ¼å¼
        if not re.match(r"^https?://[^\s/$.?#].[^\s]*$", clean_link):
            return ""

        return clean_link.strip()

    def _is_valid_url(self, url):
        """éªŒè¯URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            url_pattern = re.compile(
                r"^https?://"
                r"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|"
                r"localhost|"
                r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})"
                r"(?::\d+)?"
                r"(?:/?|[/?]\S+)$",
                re.IGNORECASE,
            )
            return url_pattern.match(url) is not None
        except Exception as e:
            self.logger.debug(f"URLéªŒè¯å¤±è´¥: {str(e)}")
            return False

    def _is_valid_subscription_link(self, url):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„V2Rayè®¢é˜…é“¾æ¥"""
        try:
            # å¯¼å…¥æ’é™¤æ¨¡å¼
            from src.config.websites import EXCLUDED_SUBSCRIPTION_PATTERNS

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ’é™¤æ¨¡å¼
            for pattern in EXCLUDED_SUBSCRIPTION_PATTERNS:
                if re.match(pattern, url, re.IGNORECASE):
                    self.logger.debug(f"é“¾æ¥è¢«æ’é™¤è§„åˆ™è¿‡æ»¤: {url}")
                    return False

            # å¿…é¡»ä»¥è®¢é˜…æ–‡ä»¶æ‰©å±•åç»“å°¾ï¼Œæ’é™¤ç½‘é¡µæ–‡ä»¶
            valid_extensions = [".txt", ".yaml", ".yml", ".json", ".sub"]
            web_extensions = [".htm", ".html", ".php", ".asp", ".jsp"]

            url_lower = url.lower()
            has_valid_ext = any(url_lower.endswith(ext) for ext in valid_extensions)
            has_web_ext = any(url_lower.endswith(ext) for ext in web_extensions)

            if not has_valid_ext or has_web_ext:
                return False

            # å¯¹äº.yaml/.yml/.jsonæ–‡ä»¶ï¼Œæ›´å®½æ¾çš„éªŒè¯ï¼ˆå› ä¸ºè¿™äº›é€šå¸¸æ˜¯ç»“æ„åŒ–é…ç½®æ–‡ä»¶ï¼‰
            if url_lower.endswith((".yaml", ".yml", ".json")):
                return True

            from urllib.parse import urlparse

            parsed = urlparse(url)
            path_part = parsed.path.lower()

            # é¦–å…ˆæ’é™¤æ˜æ˜¾çš„éV2Rayæ–‡ä»¶ï¼ˆåŸºäºæ–‡ä»¶åæ¨¡å¼ï¼Œåªæ£€æŸ¥è·¯å¾„éƒ¨åˆ†ï¼‰
            non_v2ray_patterns = [
                r".*/[^/]*clash[^/]*\.txt$",  # æ’é™¤clashç›¸å…³çš„txtæ–‡ä»¶
                r".*/[^/]*sing.*box[^/]*\.txt$",  # æ’é™¤sing-boxç›¸å…³çš„txtæ–‡ä»¶
                r".*/[^/]*config[^/]*\.txt$",  # æ’é™¤configç›¸å…³çš„txtæ–‡ä»¶
                # æ³¨æ„ï¼šyamlå’Œymlæ‰©å±•åçš„æ–‡ä»¶ç°åœ¨æ˜¯å…è®¸çš„
            ]

            for pattern in non_v2ray_patterns:
                if re.match(pattern, url, re.IGNORECASE):
                    return False

            # æ£€æŸ¥URLè·¯å¾„ä¸­æ˜¯å¦åŒ…å«V2Rayç›¸å…³å…³é”®è¯
            v2ray_keywords = [
                "v2ray",
                "sub",
                "subscribe",
                "node",
                "link",
                "vmess",
                "vless",
                "trojan",
            ]
            has_keyword = any(keyword in path_part for keyword in v2ray_keywords)

            # å¦‚æœè·¯å¾„ä¸­æ²¡æœ‰å…³é”®è¯ï¼Œæ£€æŸ¥åŸŸå
            if not has_keyword:
                domain = parsed.netloc.lower()
                domain_keywords = ["v2ray", "node", "sub", "vmess", "vless", "trojan"]
                has_domain_keyword = any(
                    keyword in domain for keyword in domain_keywords
                )

                # å¦‚æœåŸŸåä¹Ÿæ²¡æœ‰å…³é”®è¯ï¼Œåˆ™æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸è§çš„èŠ‚ç‚¹æœåŠ¡åŸŸåæ¨¡å¼
                if not has_domain_keyword:
                    # å¸¸è§çš„èŠ‚ç‚¹æœåŠ¡åŸŸåæ¨¡å¼
                    common_patterns = [
                        r".*\.mibei77\.com",
                        r".*\.freeclashnode\.com",
                        r".*node\..*",
                        r".*sub\..*",
                        r".*api\..*",
                        r".*\..*\.txt$",  # ä»»ä½•åŒ…å«æ•°å­—å’Œå­—æ¯çš„.txtæ–‡ä»¶
                    ]

                    for pattern in common_patterns:
                        if re.match(pattern, url, re.IGNORECASE):
                            return True

                    # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œåˆ™è®¤ä¸ºä¸æ˜¯V2Rayè®¢é˜…
                    return False

            # æ’é™¤æ˜æ˜¾çš„å†…å®¹è½¬æ¢ç½‘ç«™
            excluded_domains = [
                "subconverter",
                "subx",
                "sub.xeton",
                "api.v1.mk",
                "v1.mk",
                "raw.git",
                "githubusercontent.com",
                "gitlab.com",
            ]
            for domain in excluded_domains:
                if domain in parsed.netloc.lower():
                    return False

            return True

        except Exception as e:
            self.logger.warning(f"éªŒè¯è®¢é˜…é“¾æ¥æ—¶å‡ºé”™: {url} - {str(e)}")
            return False
