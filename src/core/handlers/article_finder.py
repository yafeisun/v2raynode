#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ç« æŸ¥æ‰¾å™¨ - æŸ¥æ‰¾æ–‡ç« é“¾æ¥å’Œæå–æ—¥æœŸ
"""

import re
import os
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from playwright.sync_api import sync_playwright


class ArticleFinder:
    """æ–‡ç« æŸ¥æ‰¾å™¨"""

    def __init__(self, base_url, site_name, logger, site_config):
        """
        åˆå§‹åŒ–æ–‡ç« æŸ¥æ‰¾å™¨

        Args:
            base_url: ç½‘ç«™åŸºç¡€URL
            site_name: ç½‘ç«™åç§°
            logger: æ—¥å¿—è®°å½•å™¨
            site_config: ç½‘ç«™é…ç½®
        """
        self.base_url = base_url
        self.site_name = site_name
        self.logger = logger
        self.site_config = site_config

    def find_latest_article(self, soup, target_date=None):
        """
        ä»BeautifulSoupå¯¹è±¡ä¸­æŸ¥æ‰¾æœ€æ–°æ–‡ç« URL

        Args:
            soup: BeautifulSoupå¯¹è±¡
            target_date: ç›®æ ‡æ—¥æœŸï¼ˆé»˜è®¤ä¸ºä»Šå¤©ï¼‰

        Returns:
            æ–‡ç« URLæˆ–None
        """
        if target_date is None:
            target_date = datetime.now()

        # æ”¶é›†æ‰€æœ‰åŒ…å«æ—¥æœŸçš„é“¾æ¥åŠå…¶æ—¥æœŸä¿¡æ¯
        dated_links = []
        all_links = soup.find_all("a", href=True)

        self.logger.debug(f"æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹æå–æ—¥æœŸ...")

        extracted_count = 0
        exclusion_reasons = {}  # ç»Ÿè®¡æ’é™¤åŸå› 

        for link in all_links:
            href = link.get("href")
            text = link.get_text(strip=True)
            title = link.get("title", "")  # æå–titleå±æ€§ï¼Œå¯èƒ½åŒ…å«æ—¥æœŸ

            if not href:
                continue

            # æ’é™¤å¯¼èˆªé“¾æ¥
            excluded = False
            exclusion_reason = ""
            if any(
                x in href
                for x in ["category", "tag", "page", "search", "about", "feed"]
            ):
                excluded = True
                exclusion_reason = "navigation link"
            elif href.startswith("#"):
                excluded = True
                exclusion_reason = "anchor link"
            elif len(href) < 10:
                excluded = True
                exclusion_reason = "href too short"

            if excluded:
                exclusion_reasons[exclusion_reason] = (
                    exclusion_reasons.get(exclusion_reason, 0) + 1
                )
                continue

            # å°è¯•ä»é“¾æ¥ã€æ–‡æœ¬æˆ–titleä¸­æå–æ—¥æœŸ
            link_date = self._extract_date_from_text(href, text, title)

            if link_date is not None:
                extracted_count += 1
                # è®¡ç®—ä¸ä»Šå¤©çš„å¤©æ•°å·®
                days_diff = abs((link_date.date() - target_date.date()).days)

                # æ£€æŸ¥æ˜¯å¦æ˜¯ä»Šå¤©çš„æ—¥æœŸ
                is_today = link_date.date() == target_date.date()

                dated_links.append(
                    {
                        "url": href,
                        "date": link_date,
                        "days_diff": days_diff,
                        "is_today": is_today,
                        "text": text,
                    }
                )

        self.logger.debug(
            f"æå–åˆ° {len(dated_links)} ä¸ªå¸¦æ—¥æœŸçš„é“¾æ¥ (å…±å°è¯•æå– {extracted_count} ä¸ª)"
        )

        # æ˜¾ç¤ºæ’é™¤ç»Ÿè®¡
        if exclusion_reasons:
            self.logger.debug(f"é“¾æ¥æ’é™¤ç»Ÿè®¡: {exclusion_reasons}")

        # æ˜¾ç¤ºå‰å‡ ä¸ªå¸¦æ—¥æœŸçš„é“¾æ¥
        if dated_links:
            sample = dated_links[:5]
            for item in sample:
                self.logger.debug(
                    f"  ğŸ“… æ—¥æœŸé“¾æ¥: {item['date'].strftime('%Y-%m-%d')} - {item['url'][:80]}... (æ–‡æœ¬: {item['text'][:30] if item['text'] else 'N/A'})"
                )
        else:
            # æ˜¾ç¤ºä¸€äº›æ ·ä¾‹é“¾æ¥ï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
            self.logger.warning(f"æœªæ‰¾åˆ°å¸¦æ—¥æœŸçš„é“¾æ¥ï¼Œæ˜¾ç¤ºå‰10ä¸ªé“¾æ¥æ ·ä¾‹:")
            sample_links = all_links[:10]
            for i, link in enumerate(sample_links):
                href = link.get("href", "")
                text = link.get_text(strip=True)[:50]
                self.logger.warning(f"  [{i + 1}] {href[:80]}... (æ–‡æœ¬: {text})")

        # æŒ‰æ—¥æœŸæ’åºï¼šä»Šå¤©çš„åœ¨å‰ï¼Œå…¶æ¬¡æŒ‰æ—¥æœŸæ–°æ—§
        dated_links.sort(key=lambda x: (not x["is_today"], x["days_diff"]))

        # å¦‚æœæœ‰ä»Šå¤©çš„æ—¥æœŸï¼Œè¿”å›ç¬¬ä¸€ä¸ª
        today_article = None
        latest_article = None

        for item in dated_links:
            if item["is_today"] and today_article is None:
                today_article = item
            if latest_article is None:
                latest_article = item

        if today_article:
            article_url = self._process_url(today_article["url"])
            self.logger.info(f"âœ… æ‰¾åˆ°ä»Šå¤©çš„æ–‡ç« : {article_url}")
            return article_url

        # å¦‚æœæ²¡æœ‰ä»Šå¤©çš„æ—¥æœŸï¼Œè¿”å›æœ€è¿‘çš„
        if latest_article:
            article_url = self._process_url(latest_article["url"])
            days_ago = (target_date.date() - latest_article["date"].date()).days
            if days_ago == 0:
                date_hint = "ä»Šå¤©"
            elif days_ago == 1:
                date_hint = "æ˜¨å¤©"
            else:
                date_hint = f"{days_ago}å¤©å‰"

            self.logger.info(
                f"âš ï¸ æœªæ‰¾åˆ°ä»Šå¤©çš„æ–‡ç« ï¼Œä½¿ç”¨æœ€æ–°çš„ ({date_hint} - {latest_article['date'].strftime('%Y-%m-%d')}): {article_url}"
            )
            return article_url

        # å¦‚æœæ—¥æœŸåŒ¹é…å¤±è´¥ï¼Œå°è¯•ç‰¹å®šé€‰æ‹©å™¨
        selectors = self.site_config.get("selectors", [])
        for selector in selectors:
            links = soup.select(selector)
            if links:
                href = links[0].get("href")
                if href:
                    article_url = self._process_url(href)
                    self.logger.info(f"é€šè¿‡é€‰æ‹©å™¨æ‰¾åˆ°æ–‡ç« : {article_url}")
                    return article_url

        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›None
        self.logger.warning(f"æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥")
        return None

    def _extract_date_from_text(self, href, text, title=""):
        """
        ä»é“¾æ¥URLã€æ–‡æœ¬æˆ–titleå±æ€§ä¸­æå–æ—¥æœŸ

        Args:
            href: é“¾æ¥URL
            text: é“¾æ¥æ–‡æœ¬
            title: é“¾æ¥titleå±æ€§

        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        # å¸¸è§æ—¥æœŸæ ¼å¼æ¨¡å¼
        date_patterns = [
            # URLä¸­çš„æ—¥æœŸæ ¼å¼
            r"/(\d{4})-(\d{1,2})-(\d{1,2})/",  # /2026-1-19/ æˆ– /2026-01-19/
            r"/(\d{4})/(\d{1,2})/(\d{1,2})/",  # /2026/1/19/ æˆ– /2026/01/19/
            r"/(\d{4})-(\d{1,2})-(\d{1,2})\.",  # /2026-1-19. æˆ– /2026-01-19.
            r"/(\d{4})/(\d{1,2})/(\d{1,2})\.",  # /2026/1/19. æˆ– /2026/01/19.
            # æ–‡æœ¬ä¸­çš„æ—¥æœŸæ ¼å¼ - ä¸­æ–‡æ ¼å¼
            r"(\d{1,2})æœˆ(\d{1,2})æ—¥",  # 1æœˆ19æ—¥ æˆ– 01æœˆ19æ—¥
            r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",  # 2026å¹´1æœˆ19æ—¥
            r"(\d{2})-(\d{1,2})-(\d{1,2})",  # 26-01-19 (å‡è®¾21ä¸–çºª)
            r"(\d{2})\.(\d{1,2})\.(\d{1,2})",  # 26.01.19 (å‡è®¾21ä¸–çºª)
        ]

        # åˆå¹¶æ‰€æœ‰å¯ç”¨æ–‡æœ¬
        combined_text = f"{href} {text} {title}"

        today = datetime.now()

        for pattern in date_patterns:
            match = re.search(pattern, combined_text)
            if match:
                try:
                    groups = match.groups()
                    groups_len = len(groups)

                    year = None
                    month = None
                    day = None

                    # æ ¹æ®æ¨¡å¼å’Œç»„æ•°å¤„ç†
                    if groups_len == 3:
                        # 3ä¸ªç»„ï¼šå¯èƒ½æ˜¯ URL æ ¼å¼æˆ– "å¹´/æœˆ/æ—¥" ä¸­æ–‡æ ¼å¼
                        if "æœˆ" in pattern and "å¹´" in pattern:
                            # 2026å¹´1æœˆ19æ—¥ æ ¼å¼
                            year, month, day = (
                                int(groups[0]),
                                int(groups[1]),
                                int(groups[2]),
                            )
                        else:
                            # URL æ ¼å¼: 2026-1-19 æˆ– 26-01-19
                            year = int(groups[0])
                            month = int(groups[1])
                            day = int(groups[2])

                            # å¤„ç†ä¸¤ä½æ•°å¹´ä»½
                            if year < 100:
                                year = 2000 + year

                    elif groups_len == 2:
                        # 2ä¸ªç»„ï¼šåªæœ‰æœˆæ—¥çš„ä¸­æ–‡æ ¼å¼ (å¦‚ 1æœˆ18æ—¥)
                        if "æœˆ" in pattern:
                            year = today.year
                            month = int(groups[0])
                            day = int(groups[1])
                        else:
                            # å…¶ä»–2ç»„æ ¼å¼ï¼Œä¸å¤„ç†
                            continue

                    else:
                        # ä¸æ”¯æŒçš„ç»„æ•°
                        continue

                    # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                    if (
                        year
                        and month
                        and day
                        and 2020 <= year <= 2030
                        and 1 <= month <= 12
                        and 1 <= day <= 31
                    ):
                        return datetime(year, month, day)

                except (ValueError, TypeError):
                    continue

        return None

    def _process_url(self, url):
        """å¤„ç†URLï¼Œå°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL"""
        if url.startswith("/"):
            return urljoin(self.base_url, url)
        return url

    def fetch_with_playwright(self, target_date=None):
        """
        ä½¿ç”¨Playwrightæµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–é¡µé¢å†…å®¹ï¼ˆç¦ç”¨ä»£ç†ï¼‰

        Args:
            target_date: ç›®æ ‡æ—¥æœŸ

        Returns:
            æ–‡ç« URLæˆ–None
        """
        try:
            self.logger.info(f"å¯åŠ¨æµè§ˆå™¨è®¿é—®: {self.base_url} (ç¦ç”¨ä»£ç†)")

            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=True,
                    args=["--no-sandbox", "--disable-dev-shm-usage"],
                )
                context = browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    viewport={"width": 1920, "height": 1080},
                    locale="zh-CN",
                )
                page = context.new_page()
                # ä½¿ç”¨ domcontentloaded é¿å… networkidle è¶…æ—¶
                page.goto(self.base_url, wait_until="domcontentloaded", timeout=30000)
                # ç­‰å¾…é¢å¤–æ—¶é—´è®©JSæ‰§è¡Œ
                page.wait_for_timeout(3000)
                content = page.content()
                browser.close()

            self.logger.info(f"æµè§ˆå™¨è·å–åˆ° {len(content)} å­—èŠ‚å†…å®¹")

            # ä¿å­˜è°ƒè¯•HTML
            debug_dir = os.path.join(os.getcwd(), "data", "debug")
            os.makedirs(debug_dir, exist_ok=True)
            debug_file = os.path.join(
                debug_dir,
                f"debug_{self.site_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
            )
            try:
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(content)
                self.logger.info(
                    f"ğŸ’¾ ä¿å­˜è°ƒè¯•HTMLåˆ°: {debug_file} ({len(content)} bytes)"
                )
            except Exception as e:
                self.logger.warning(f"ä¿å­˜è°ƒè¯•HTMLå¤±è´¥: {str(e)}")

            soup = BeautifulSoup(content, "html.parser")
            article_url = self.find_latest_article(soup, target_date)

            if article_url:
                self.logger.info(f"âœ… æ‰¾åˆ°æ–‡ç« URL: {article_url}")
            else:
                self.logger.warning(f"âŒ æœªæ‰¾åˆ°æ–‡ç« URL")

            return article_url

        except Exception as e:
            self.logger.error(f"Playwrightè®¿é—®å¤±è´¥: {str(e)}")
            return None